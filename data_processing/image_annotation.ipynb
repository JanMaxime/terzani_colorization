{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image annotation script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will serve as a prototype to create a script to annotate all the images of Terzani collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the client library\n",
    "\n",
    "If the Google cloud vision library is not installed already, install it.\n",
    "\n",
    "If you have python environment use\n",
    "\n",
    "```shell\n",
    "pip install --upgrade google-cloud-vision\n",
    "```\n",
    "\n",
    "If you have conda environment use\n",
    "\n",
    "```shell\n",
    "conda install -c conda-forge google-cloud-vision\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing other libraries\n",
    "\n",
    "Install `dotenv` to get the environment variables\n",
    "\n",
    "If you have python environment use\n",
    "\n",
    "```shell\n",
    "pip install python-dotenv\n",
    "```\n",
    "\n",
    "If you have conda environment use\n",
    "\n",
    "```shell\n",
    "conda install -c conda-forge python-dotenvn\n",
    "```\n",
    "\n",
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the standard libraries\n",
    "import os, io, pickle, random, json\n",
    "## Import Vison API related libraries\n",
    "from google.cloud import vision\n",
    "from google.cloud.vision import types\n",
    "## Import dotenv library to get environment variables\n",
    "from dotenv import load_dotenv\n",
    "# Import urllib to read images\n",
    "import urllib.request as ur\n",
    "# Import pymango to inset data into mangodb\n",
    "import pymongo\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the service account credentials to use the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "GOOGLE_APPLICATION_CREDENTIALS = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = GOOGLE_APPLICATION_CREDENTIALS\n",
    "\n",
    "MANGO_CLIENT_URI = os.getenv('MONGO_URI')\n",
    "os.environ['MANGO_CLIENT_URI'] = MANGO_CLIENT_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of Client to access the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = vision.ImageAnnotatorClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image selection\n",
    "\n",
    "As this is a prototyping script we shall select 10 images randomly each from the color and monochrome photos (using the already created pickle files)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic class to store an image and its IIIF representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Terzani_Photo(object):\n",
    "    def __init__(self, iiif, photo):\n",
    "        self.iiif = iiif\n",
    "        self.photo = photo\n",
    "        \n",
    "    def get_photo_link(self):\n",
    "        return self.iiif[\"images\"][0][\"resource\"][\"@id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_images = 10\n",
    "\n",
    "# loading the color photos\n",
    "color_photos = pickle.load(open(\"terzani_recto_iiif_color.pickle\", \"rb\" ))\n",
    "# randomly selecting 10 images\n",
    "color_photos = random.sample(color_photos, number_of_images)\n",
    "\n",
    "# loading the monochrome photos\n",
    "bw_photos = pickle.load(open(\"terzani_recto_iiif_color.pickle\", \"rb\" ))\n",
    "# randomly selecting 10 images\n",
    "bw_photos = random.sample(bw_photos, number_of_images)\n",
    "\n",
    "all_photos = color_photos + bw_photos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling the Vision API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [01:27<00:00,  4.38s/it]\n"
     ]
    }
   ],
   "source": [
    "annotated_images = dict() # We store information about each image in a dictionary to later transform into json.\n",
    "failed_images = dict() # We store information about images that are failed to be annotated by google api.\n",
    "for img in tqdm(all_photos):\n",
    "\n",
    "    # if the image is already not present in the either annotated and failed dictionaries\n",
    "    if img.iiif[\"label\"] not in annotated_images and img.iiif[\"label\"] not in failed_images:\n",
    "\n",
    "        img_lbl = img.iiif[\"label\"]\n",
    "                \n",
    "        image_data = ur.urlopen(img.get_photo_link()).read()\n",
    "        image = types.Image(content=image_data)\n",
    "        \n",
    "        # call the goole vision api to get the annotations of various types\n",
    "        response = client.annotate_image({\n",
    "            'image': image,\n",
    "            'features': [{'type': vision.enums.Feature.Type.LANDMARK_DETECTION}, \n",
    "                         {'type': vision.enums.Feature.Type.LOGO_DETECTION},\n",
    "                         {'type': vision.enums.Feature.Type.LABEL_DETECTION},\n",
    "                         {'type': vision.enums.Feature.Type.TEXT_DETECTION},\n",
    "                         {'type': vision.enums.Feature.Type.OBJECT_LOCALIZATION},\n",
    "                         {'type': vision.enums.Feature.Type.WEB_DETECTION}],})\n",
    "        \n",
    "        # check if there is any error returned by the api\n",
    "        if response.error.code != 0:\n",
    "            failed_images[img_lbl] = {}\n",
    "            failed_images[img_lbl][\"error\"] = [response.error.code, response.error.message]\n",
    "        else:\n",
    "            annotated_images[img_lbl] = {}\n",
    "        \n",
    "            # store the iiif description\n",
    "            annotated_images[img_lbl][\"iiif\"] = img.iiif\n",
    "            \n",
    "            tags = list() # A list to store tags for the image originating from label and web detection\n",
    "\n",
    "            # We store the labels and webentities in a list called tags\n",
    "\n",
    "            tags.extend([lbl.description for lbl in response.label_annotations])\n",
    "            tags.extend([weben.description for weben in response.web_detection.web_entities])\n",
    "\n",
    "            # store the generated tags into the dictionary.\n",
    "            # The list is made into set and converedted back into to list to eliminate any duplicate tags \n",
    "            annotated_images[img_lbl][\"tags\"] = list(set(tags))\n",
    "\n",
    "            obj_boxes = {} # this dictionary will store the information of annotations along with bounding boxes.\n",
    "            # The key will the the name to identify the annotation and the value be a list of list of tuples with coordinates\n",
    "            # for the bounding box. It would be a list of list to store coordinates for different boxes for same tag\n",
    "\n",
    "            for lndmk in response.landmark_annotations:\n",
    "                if lndmk.description not in obj_boxes:\n",
    "                    obj_boxes[lndmk.description] = list()\n",
    "                ulx, uly, box_width, box_height = None, None, None, None\n",
    "                ulx, uly = lndmk.bounding_poly.vertices[3].x, lndmk.bounding_poly.vertices[3].y\n",
    "                box_width = abs(lndmk.bounding_poly.vertices[2].x - lndmk.bounding_poly.vertices[3].x)\n",
    "                box_height = abs(lndmk.bounding_poly.vertices[2].y - lndmk.bounding_poly.vertices[1].y)\n",
    "                if (ulx and uly and box_width and box_height) is not None:\n",
    "                    vert = [ulx, uly, box_width, box_height] \n",
    "                    obj_boxes[lndmk.description].append(vert)    \n",
    "\n",
    "            for lgo in response.logo_annotations:\n",
    "                if lgo.description not in obj_boxes:\n",
    "                    obj_boxes[lgo.description] = list()\n",
    "                ulx, uly, box_width, box_height = None, None, None, None\n",
    "                ulx, uly = lgo.bounding_poly.vertices[3].x, lgo.bounding_poly.vertices[3].y\n",
    "                box_width = abs(lgo.bounding_poly.vertices[2].x - lgo.bounding_poly.vertices[3].x)\n",
    "                box_height = abs(lgo.bounding_poly.vertices[2].y - lgo.bounding_poly.vertices[1].y)\n",
    "                if (ulx and uly and box_width and box_height) is not None:\n",
    "                    vert = [ulx, uly, box_width, box_height]\n",
    "                    obj_boxes[lgo.description].append(vert)\n",
    "\n",
    "            if len(response.localized_object_annotations) > 0:\n",
    "                img_width, img_height = img.iiif[\"width\"], img.iiif[\"height\"]\n",
    "            for lobj in response.localized_object_annotations:\n",
    "                if lobj.name not in obj_boxes:\n",
    "                    obj_boxes[lobj.name] = list()\n",
    "                ulx, uly, box_width, box_height = None, None, None, None\n",
    "                ulx, uly = lobj.bounding_poly.normalized_vertices[0].x * img_width, lobj.bounding_poly.normalized_vertices[0].y * img_height\n",
    "                box_width = (lobj.bounding_poly.normalized_vertices[1].x - lobj.bounding_poly.normalized_vertices[0].x) * img_width\n",
    "                box_height = (lobj.bounding_poly.normalized_vertices[3].y - lobj.bounding_poly.normalized_vertices[0].y) * img_height\n",
    "                if (ulx and uly and box_width and box_height) is not None:\n",
    "                    vert = [ulx, uly, box_width, box_height]\n",
    "                    obj_boxes[lobj.name].append(vert)\n",
    "\n",
    "            for txt in response.text_annotations:\n",
    "                modified_text = txt.description.replace(\".\", \"_\")\n",
    "                if modified_text not in obj_boxes:\n",
    "                    obj_boxes[modified_text] = list()\n",
    "                ulx, uly, box_width, box_height = None, None, None, None\n",
    "                ulx, uly = txt.bounding_poly.vertices[3].x, txt.bounding_poly.vertices[3].y\n",
    "                box_width = abs(txt.bounding_poly.vertices[2].x - txt.bounding_poly.vertices[3].x)\n",
    "                box_height = abs(txt.bounding_poly.vertices[2].y - txt.bounding_poly.vertices[1].y)\n",
    "                if (ulx and uly and box_width and box_height) is not None:\n",
    "                    vert = [ulx, uly, box_width, box_height]\n",
    "                    obj_boxes[modified_text].append(vert)\n",
    "            # store the generated object boxes into the dictionary.\n",
    "            annotated_images[img_lbl][\"obj_boxes\"] = obj_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the dictionaries to JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('annotated_images.json', 'w') as fp:\n",
    "    json.dump(annotated_images, fp, indent=4)\n",
    "\n",
    "if len(failed_images) > 0:\n",
    "    print(\"There are failed images\")\n",
    "    with open('failed_images.json', 'w') as fp:\n",
    "        json.dump(failed_images, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting the data into Mangodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a client to work with mango db\n",
    "mangoclient = pymongo.MongoClient(MANGO_CLIENT_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the <terzani_photos> database\n",
    "mango_db = mangoclient[\"terzani_photos\"]\n",
    "# creating a new collection named <sample_annotations>\n",
    "mango_collection = mango_db[\"sample_annotations\"]\n",
    "# inserting the dictionary into the db\n",
    "for label, annotations in annotated_images.items():\n",
    "    mango_collection.insert_one(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
